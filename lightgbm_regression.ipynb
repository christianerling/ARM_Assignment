{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import timedelta\n",
    "from itertools import chain\n",
    "from time import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from bayes_opt import BayesianOptimization\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_error(actual, predicted):\n",
    "    res = np.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = pd.read_json(\"data/owi-covid-values_imputed.json\")\n",
    "x_data = data_preprocessed.loc[:, data_preprocessed.columns != \"new_deaths_smoothed\"]\n",
    "y_data = data_preprocessed.loc[:, data_preprocessed.columns == \"new_deaths_smoothed\"]\n",
    "\n",
    "# Convert the pivot columns for the location back to location column i.o. to speed up the execution on GPU\n",
    "eu_countries = [\"Austria\", \"Belgium\", \"Bulgaria\", \"Croatia\", \"Cyprus\", \"Czech Republic\", \"Denmark\", \"Estonia\",\n",
    "                \"Finland\", \"France\", \"Germany\", \"Greece\", \"Hungary\", \"Ireland\", \"Italy\", \"Latvia\", \"Luxembourg\",\n",
    "                \"Lithuania\", \"Malta\", \"Netherlands\", \"Poland\", \"Portugal\", \"Romania\", \"Slovenia\", \"Slovakia\",\n",
    "                \"Spain\", \"Sweden\", \"United Kingdom\"]\n",
    "eu_countries = list(map(lambda x: \"location_\" + str(x), eu_countries))\n",
    "original_back = list(data_preprocessed[eu_countries].idxmax(axis=1))\n",
    "original_back = list(map(lambda x: x.replace(\"location_\", \"\"), original_back))\n",
    "\n",
    "data_preprocessed = data_preprocessed.drop(eu_countries, axis=1)\n",
    "data_preprocessed[\"location\"] = original_back\n",
    "data_preprocessed[\"location\"] = data_preprocessed[\"location\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part used for searching the hyperparameters\n",
    "\n",
    "# Activate with Multiprocessing, params, and 5 fold CV\n",
    "# Objective Function\n",
    "def lgb_r2_score(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'r2', r2_score(labels, preds), True\n",
    "\n",
    "\n",
    "dtrain = lgb.Dataset(data=x_data, label=y_data)\n",
    "\n",
    "\n",
    "def bayesion_opt_lgbm(X, y, init_iter=3, n_iters=7, random_state=11, seed=101, num_iterations=100):\n",
    "    dtrain = lgb.Dataset(data=X, label=y)\n",
    "\n",
    "    def lgb_r2_score(preds, dtrain):\n",
    "        labels = dtrain.get_label()\n",
    "        return 'r2', r2_score(labels, preds), True\n",
    "\n",
    "    # Objective Function\n",
    "    def hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight):\n",
    "        params = {'application': 'regression', 'num_iterations': num_iterations,\n",
    "                  'learning_rate': 0.05, 'early_stopping_round': 50,\n",
    "                  'metric': 'lgb_r2_score'}  # Default parameters\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        params['verbose'] = -1\n",
    "        cv_results = lgb.cv(params, dtrain, nfold=5, seed=seed, categorical_feature=[], stratified=False,\n",
    "                            verbose_eval=None, feval=lgb_r2_score)\n",
    "        # print(cv_results)\n",
    "        return np.max(cv_results['r2-mean'])\n",
    "\n",
    "    # Domain space-- Range of hyperparameters\n",
    "    pds = {'num_leaves': (80, 100),\n",
    "           'feature_fraction': (0.1, 0.9),\n",
    "           'bagging_fraction': (0.8, 1),\n",
    "           'max_depth': (17, 25),\n",
    "           'min_split_gain': (0.001, 0.1),\n",
    "           'min_child_weight': (10, 25)\n",
    "           }\n",
    "\n",
    "    # Surrogate model\n",
    "    optimizer = BayesianOptimization(hyp_lgbm, pds, random_state=random_state)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.maximize(init_points=init_iter, n_iter=n_iters)\n",
    "\n",
    "t1 = time()\n",
    "print(\"\\n\\nLightGBM:\")\n",
    "print(\"\\nBayesian Optimization:\")\n",
    "bayesion_opt_lgbm(x_data, y_data, init_iter=10, n_iters=200, random_state=77, seed=101, num_iterations=400)\n",
    "t2 = time()\n",
    "print(f\"\\n\\nExecution Time {timedelta(seconds=t2 - t1)}\")\n",
    "data = pd.read_excel(\"data/lightgbm_bayesian_optimization.xlsx\")\n",
    "fig = make_subplots(rows=1, cols=6,\n",
    "                    subplot_titles=[\"bagging_fraction\", \"feature_fraction\", \"max_depth\", \"min_child_weight\",\n",
    "                                    \"min_split_gain\", \"num_leaves\"])\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data[\"bagging_fraction\"], y=data[\"target\"], mode='markers', name=\"bagging_fraction\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data[\"feature_fraction\"], y=data[\"target\"], mode='markers', name=\"feature_fraction\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data[\"max_depth\"], y=data[\"target\"], mode='markers', name=\"max_depth\"),\n",
    "    row=1, col=3\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data[\"min_child_weight\"], y=data[\"target\"], mode='markers', name=\"min_child_weight\"),\n",
    "    row=1, col=4\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data[\"min_split_gain\"], y=data[\"target\"], mode='markers', name=\"min_split_gain\"),\n",
    "    row=1, col=5\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data[\"num_leaves\"], y=data[\"target\"], mode='markers', name=\"num_leaves\"),\n",
    "    row=1, col=6\n",
    ")\n",
    "fig.update_layout(height=600, width=1500, title_text=\"Hyperparameter for Target Variable R\\u00b2\")\n",
    "plotly.offline.plot(fig, filename='data/lightgbm_bayesian_optimization_result.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute testrun 1200 times\n",
    "mean_result = []\n",
    "# max_bin=63 add below if device is GPU\n",
    "lm = lgb.LGBMRegressor(bagging_fraction=0.9133, feature_fraction=0.5429, max_depth=int(24.99),\n",
    "                       min_child_weight=11.66, min_split_gain=0.008908, num_leaves=int(84.88),\n",
    "                       application=\"regression\", num_iterations=200, learning_rate=0.05, metric='lgb_r2_score',\n",
    "                       device=\"cpu\", n_jobs=-1, gpu_use_dp=False, categorical_column=24)\n",
    "predicted = []\n",
    "true_vals = []\n",
    "feature_imp = dict()\n",
    "for i in tqdm(range(1200)):\n",
    "    cv_result = []\n",
    "    indices = []\n",
    "    s_split = ShuffleSplit(n_splits=5, test_size=0.2, train_size=0.8)\n",
    "    for train_index, test_index in s_split.split(x_data):\n",
    "        indices.append([train_index, test_index])\n",
    "        X_train, X_test = x_data.iloc[train_index], x_data.iloc[test_index]\n",
    "        y_train, y_test = y_data.iloc[train_index], y_data.iloc[test_index]\n",
    "        t1 = time()\n",
    "        lm.fit(X_train, y_train)\n",
    "        coeff = np.abs(lm.feature_importances_)\n",
    "        rel_func = lambda x: x / np.sum(coeff)\n",
    "        coeff = rel_func(coeff)\n",
    "\n",
    "        for counter, column in enumerate(x_data.columns):\n",
    "            if column in feature_imp.keys():\n",
    "                feature_imp[column].append(coeff[counter])\n",
    "            else:\n",
    "                feature_imp.update({column: [coeff[counter]]})\n",
    "        y_pred = lm.predict(X_test)\n",
    "        t2 = time()\n",
    "        predicted.append(y_pred.tolist())\n",
    "        true_vals.append(y_test[\"new_deaths_smoothed\"].tolist())\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        cv_result.append([r2, mse, rmse, mae, mape, t2 - t1])\n",
    "    means = list(np.mean(np.array(cv_result), axis=0))\n",
    "    mean_result.append(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Feature Importances and Export results\n",
    "for key, value in feature_imp.items():\n",
    "    feature_imp[key] = np.mean(feature_imp[key])\n",
    "imp_coef = pd.Series(feature_imp)\n",
    "imp_coef = pd.DataFrame(imp_coef).reset_index()\n",
    "imp_coef.columns = [\"Feature\", \"Value\"]\n",
    "imp_coef = imp_coef.sort_values(by=\"Value\", ascending=False)\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=imp_coef)\n",
    "plt.title('Relative LightGBM Feature Importance (mean over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances-01.png', dpi=200)\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(mean_result, columns=[\"R2\", \"MSE\", \"RMSE\", \"MAE\", \"MAPE\", \"Execution Time\"]).to_excel(\n",
    "    \"data/lightgbm_cv_run.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Prediction Interval\n",
    "predicted = list(chain.from_iterable(predicted))\n",
    "true_vals = list(chain.from_iterable(true_vals))\n",
    "regression_res_collected = dict()\n",
    "\n",
    "for counter, true_val in enumerate(true_vals):\n",
    "    if true_val in regression_res_collected.keys():\n",
    "        regression_res_collected[true_val].append(predicted[counter])\n",
    "    else:\n",
    "        regression_res_collected.update({true_val: [predicted[counter]]})\n",
    "\n",
    "regression_res_collected = dict(sorted(regression_res_collected.items()))\n",
    "\n",
    "for key, value in regression_res_collected.items():\n",
    "    regression_res_collected[key] = [min(regression_res_collected[key]), max(regression_res_collected[key])]\n",
    "\n",
    "max_pred = []\n",
    "min_pred = []\n",
    "for key, value in regression_res_collected.items():\n",
    "    max_pred.append(value[1])\n",
    "    min_pred.append(value[0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "real_vals = list(regression_res_collected.keys())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.fill_between(real_vals, min_pred, max_pred, alpha=1.0, interpolate=True, color=\"red\", label='Prediction Interval')\n",
    "ax.plot([min(true_vals), max(true_vals)], [min(true_vals), max(true_vals)], color=\"blue\", linestyle='--', lw=4,\n",
    "        label=\"Ideal Prediction\")\n",
    "ax.set_xlabel(\"Real new_deaths_smoothed\")\n",
    "ax.set_ylabel(\"Predicted new_deaths_smoothed\")\n",
    "fig.suptitle('LightGBM Regression', fontsize=16)\n",
    "plt.legend(loc=\"upper left\", frameon=False)\n",
    "plt.savefig(\"data/lightgbm_cv_results.png\", dpi=250)\n",
    "plt.show()"
   ]
  }
 ]
}